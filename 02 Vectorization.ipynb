{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161faa0-6a28-4be7-aa44-32b3f3e94233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8301ec91-2389-4afc-90d4-eb25fb00e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"./data/input/train.csv\")\n",
    "data_df.drop(columns=['uuid', 'title', 'author', 'Keywords'], inplace=True, axis=1)\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27355e49-65d3-4cba-806c-d90f2fa0a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataframe(row):\n",
    "    row['tokenized'] = nltk.tokenize.word_tokenize(row['abstract'])\n",
    "    return row\n",
    "\n",
    "data_df['tokenized'] = [list() for _ in range(data_df.shape[0])]\n",
    "data_df = data_df.apply(tokenize_dataframe, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc86eb9d-9472-4cb1-8960-53d0ea4fc5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe10f016-418f-4b5b-abdf-471b1a7b9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2e7297-913c-457d-9409-bba9525561ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c8114-1e57-41cc-a258-cd014729cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "do_remove_punct = True\n",
    "do_lemmatize = True\n",
    "do_lowercase = True\n",
    "do_remove_stop = True\n",
    "\n",
    "def custom_tokenize(row):\n",
    "    text = row['abstract']\n",
    "    \n",
    "    # 01 - Punctuations\n",
    "    if do_remove_punct:\n",
    "        text = re.sub(r'([^\\w\\s])', '', text)\n",
    "\n",
    "    # 02 - Lemmatization\n",
    "    if do_lemmatize:\n",
    "        tokens_list = nltk.word_tokenize(text)\n",
    "        text = ' '.join([lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in tokens_list])\n",
    "\n",
    "    # 03 - Lowercasing\n",
    "    if do_lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    # 04 - Removing stop words (i.e. grammar defining words, not adding value to main topic)\n",
    "    if do_remove_stop:\n",
    "        text = ' '.join([t for t in text.split() if t not in stopwords])\n",
    "\n",
    "    row['custom_tokenized'] = text.split()\n",
    "    return row\n",
    "\n",
    "data_df['custom_tokenized'] = [list() for _ in range(data_df.shape[0])]\n",
    "data_df = data_df.apply(custom_tokenize, axis=1)\n",
    "\n",
    "data_df.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
