{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3aa3ce9-a0f4-4f08-84a6-57360e328f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06b1514-9387-4c46-8745-d8010206f9e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>abstract</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Accessible Visual Artworks for Blind and Visua...</td>\n",
       "      <td>Quero, Luis Cavazos; Bartolome, Jorge Iranzo; ...</td>\n",
       "      <td>Despite the use of tactile graphics and audio ...</td>\n",
       "      <td>accessibility technology; multimodal interacti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Seizure Detection and Prediction by Parallel M...</td>\n",
       "      <td>Li, Chenqi; Lammie, Corey; Dong, Xuening; Amir...</td>\n",
       "      <td>During the past two decades, epileptic seizure...</td>\n",
       "      <td>CNN; Seizure Detection; Seizure Prediction; EE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Fast ScanNet: Fast and Dense Analysis of Multi...</td>\n",
       "      <td>Lin, Huangjing; Chen, Hao; Graham, Simon; Dou,...</td>\n",
       "      <td>Lymph node metastasis is one of the most impor...</td>\n",
       "      <td>Histopathology image analysis; computational p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Long-Term Effectiveness of Antiretroviral Ther...</td>\n",
       "      <td>Huang, Peng; Tan, Jingguang; Ma, Wenzhe; Zheng...</td>\n",
       "      <td>In order to assess the effectiveness of the Ch...</td>\n",
       "      <td>HIV; ART; mortality; observational cohort stud...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Real-Time Facial Affective Computing on Mobile...</td>\n",
       "      <td>Guo, Yuanyuan; Xia, Yifan; Wang, Jing; Yu, Hui...</td>\n",
       "      <td>Convolutional Neural Networks (CNNs) have beco...</td>\n",
       "      <td>facial affective computing; convolutional neur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uuid                                              title  \\\n",
       "0     0  Accessible Visual Artworks for Blind and Visua...   \n",
       "1     1  Seizure Detection and Prediction by Parallel M...   \n",
       "2     2  Fast ScanNet: Fast and Dense Analysis of Multi...   \n",
       "3     3  Long-Term Effectiveness of Antiretroviral Ther...   \n",
       "4     4  Real-Time Facial Affective Computing on Mobile...   \n",
       "\n",
       "                                              author  \\\n",
       "0  Quero, Luis Cavazos; Bartolome, Jorge Iranzo; ...   \n",
       "1  Li, Chenqi; Lammie, Corey; Dong, Xuening; Amir...   \n",
       "2  Lin, Huangjing; Chen, Hao; Graham, Simon; Dou,...   \n",
       "3  Huang, Peng; Tan, Jingguang; Ma, Wenzhe; Zheng...   \n",
       "4  Guo, Yuanyuan; Xia, Yifan; Wang, Jing; Yu, Hui...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Despite the use of tactile graphics and audio ...   \n",
       "1  During the past two decades, epileptic seizure...   \n",
       "2  Lymph node metastasis is one of the most impor...   \n",
       "3  In order to assess the effectiveness of the Ch...   \n",
       "4  Convolutional Neural Networks (CNNs) have beco...   \n",
       "\n",
       "                                            Keywords  label  \n",
       "0  accessibility technology; multimodal interacti...      0  \n",
       "1  CNN; Seizure Detection; Seizure Prediction; EE...      1  \n",
       "2  Histopathology image analysis; computational p...      1  \n",
       "3  HIV; ART; mortality; observational cohort stud...      0  \n",
       "4  facial affective computing; convolutional neur...      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"./data/input/train.csv\")\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9d7310-bb3a-4ba7-a473-8588e0c13ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2)\n"
     ]
    }
   ],
   "source": [
    "data_df.drop(columns=['uuid', 'title', 'author', 'Keywords'], inplace=True, axis=1)\n",
    "print(data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a26cb-98e0-4be4-b0f7-8655c4b840d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92194a58-57b2-4e89-a38b-e93784744f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f0898c-7b9f-4555-bf3e-689cd402611e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2677a16-f79c-4041-bfb7-bd3ae3d8af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "870e699f-d650-4abe-aefc-1023f26eb5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>label</th>\n",
       "      <th>custom_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Despite the use of tactile graphics and audio ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[despite, use, tactile, graphics, audio, guide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>During the past two decades, epileptic seizure...</td>\n",
       "      <td>1</td>\n",
       "      <td>[past, two, decades, epileptic, seizure, detec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lymph node metastasis is one of the most impor...</td>\n",
       "      <td>1</td>\n",
       "      <td>[lymph, node, metastasis, one, important, indi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In order to assess the effectiveness of the Ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>[order, assess, effectiveness, chinese, govern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Convolutional Neural Networks (CNNs) have beco...</td>\n",
       "      <td>0</td>\n",
       "      <td>[convolutional, neural, networks, cnns, become...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Previously we showed the generation of a prote...</td>\n",
       "      <td>1</td>\n",
       "      <td>[previously, showed, generation, protein, trap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Facial emotion recognition (FER) is a field of...</td>\n",
       "      <td>0</td>\n",
       "      <td>[facial, emotion, recognition, fer, field, res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This paper proposes a machine learning model b...</td>\n",
       "      <td>0</td>\n",
       "      <td>[paper, proposes, machine, learning, model, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Most current state-of-the-art blind image qual...</td>\n",
       "      <td>0</td>\n",
       "      <td>[current, state, art, blind, image, quality, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Surgical workflow recognition has numerous pot...</td>\n",
       "      <td>1</td>\n",
       "      <td>[surgical, workflow, recognition, numerous, po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Due to the lack of available episomal plasmid,...</td>\n",
       "      <td>1</td>\n",
       "      <td>[due, lack, available, episomal, plasmid, impr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>In contrast to numerous studies on spermatozoa...</td>\n",
       "      <td>1</td>\n",
       "      <td>[contrast, numerous, studies, spermatozoa, len...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Study on finger knuckle patterns has attracted...</td>\n",
       "      <td>0</td>\n",
       "      <td>[study, finger, knuckle, patterns, attracted, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>In the present work, we follow in chronologica...</td>\n",
       "      <td>0</td>\n",
       "      <td>[present, work, follow, chronological, order, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Curved text detection is a difficult problem t...</td>\n",
       "      <td>0</td>\n",
       "      <td>[curved, text, detection, difficult, problem, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Refugees are a vulnerable, growing population ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[refugees, vulnerable, growing, population, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Weakly supervised object detection (WSOD) usin...</td>\n",
       "      <td>0</td>\n",
       "      <td>[weakly, supervised, object, detection, wsod, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>This paper presents a novel, fast, group-wise ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[paper, presents, novel, fast, group, wise, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Limited by the electrostatic interaction, the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[limited, electrostatic, interaction, oxidatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The integration of analytical strategies and g...</td>\n",
       "      <td>0</td>\n",
       "      <td>[integration, analytical, strategies, global, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             abstract  label  \\\n",
       "0   Despite the use of tactile graphics and audio ...      0   \n",
       "1   During the past two decades, epileptic seizure...      1   \n",
       "2   Lymph node metastasis is one of the most impor...      1   \n",
       "3   In order to assess the effectiveness of the Ch...      0   \n",
       "4   Convolutional Neural Networks (CNNs) have beco...      0   \n",
       "5   Previously we showed the generation of a prote...      1   \n",
       "6   Facial emotion recognition (FER) is a field of...      0   \n",
       "7   This paper proposes a machine learning model b...      0   \n",
       "8   Most current state-of-the-art blind image qual...      0   \n",
       "9   Surgical workflow recognition has numerous pot...      1   \n",
       "10  Due to the lack of available episomal plasmid,...      1   \n",
       "11  In contrast to numerous studies on spermatozoa...      1   \n",
       "12  Study on finger knuckle patterns has attracted...      0   \n",
       "13  In the present work, we follow in chronologica...      0   \n",
       "14  Curved text detection is a difficult problem t...      0   \n",
       "15  Refugees are a vulnerable, growing population ...      1   \n",
       "16  Weakly supervised object detection (WSOD) usin...      0   \n",
       "17  This paper presents a novel, fast, group-wise ...      1   \n",
       "18  Limited by the electrostatic interaction, the ...      1   \n",
       "19  The integration of analytical strategies and g...      0   \n",
       "\n",
       "                                     custom_tokenized  \n",
       "0   [despite, use, tactile, graphics, audio, guide...  \n",
       "1   [past, two, decades, epileptic, seizure, detec...  \n",
       "2   [lymph, node, metastasis, one, important, indi...  \n",
       "3   [order, assess, effectiveness, chinese, govern...  \n",
       "4   [convolutional, neural, networks, cnns, become...  \n",
       "5   [previously, showed, generation, protein, trap...  \n",
       "6   [facial, emotion, recognition, fer, field, res...  \n",
       "7   [paper, proposes, machine, learning, model, ba...  \n",
       "8   [current, state, art, blind, image, quality, a...  \n",
       "9   [surgical, workflow, recognition, numerous, po...  \n",
       "10  [due, lack, available, episomal, plasmid, impr...  \n",
       "11  [contrast, numerous, studies, spermatozoa, len...  \n",
       "12  [study, finger, knuckle, patterns, attracted, ...  \n",
       "13  [present, work, follow, chronological, order, ...  \n",
       "14  [curved, text, detection, difficult, problem, ...  \n",
       "15  [refugees, vulnerable, growing, population, co...  \n",
       "16  [weakly, supervised, object, detection, wsod, ...  \n",
       "17  [paper, presents, novel, fast, group, wise, re...  \n",
       "18  [limited, electrostatic, interaction, oxidatio...  \n",
       "19  [integration, analytical, strategies, global, ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "do_remove_punct = True\n",
    "do_lemmatize = False\n",
    "do_lowercase = True\n",
    "do_remove_stop = True\n",
    "do_remove_nums = True\n",
    "\n",
    "def custom_tokenize(row):\n",
    "    text = row['abstract']\n",
    "    \n",
    "    # 01 - Punctuations\n",
    "    if do_remove_punct:\n",
    "        text = re.sub(r'([^\\w\\s])', ' ', text)\n",
    "\n",
    "    # 02 - Lemmatization\n",
    "    if do_lemmatize:\n",
    "        tokens_list = nltk.word_tokenize(text)\n",
    "        text = ' '.join([lemmatizer.lemmatize(t, get_wordnet_pos(t)) for t in tokens_list])\n",
    "\n",
    "    # 03 - Lowercasing\n",
    "    if do_lowercase:\n",
    "        text = text.lower()\n",
    "\n",
    "    # 04 - Removing stop words (i.e. grammar defining words, not adding value to main topic)\n",
    "    if do_remove_stop:\n",
    "        text = ' '.join([t for t in text.split() if t not in stopwords])\n",
    "\n",
    "    # 05 - Removing numbers\n",
    "    if do_remove_nums:\n",
    "        text = re.sub(r'\\b[0-9]+\\b', ' ', text)\n",
    "\n",
    "    # Removing redundant spaces\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "\n",
    "    row['custom_tokenized'] = text.split()\n",
    "    return row\n",
    "\n",
    "data_df['custom_tokenized'] = [list() for _ in range(data_df.shape[0])]\n",
    "data_df = data_df.apply(custom_tokenize, axis=1)\n",
    "\n",
    "data_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a0ffec-57e7-425b-b000-abdc3d4ed2c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbb35909-61f9-441e-a5b1-4905d410d98e",
   "metadata": {},
   "source": [
    "# Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040ed118-c9f1-456c-8b1f-4210b9dcfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_df[['abstract', 'custom_tokenized']]\n",
    "Y = data_df[['label']]\n",
    "\n",
    "X_train_text, X_val_text, Y_train, Y_val = train_test_split(X, \n",
    "                                                            Y, \n",
    "                                                            test_size=0.2, \n",
    "                                                            shuffle=True, \n",
    "                                                            random_state=42\n",
    "                                                           )\n",
    "del X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1397cda-8414-4b9a-929f-8168f6a4ea48",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_docs_train = X_train_text['custom_tokenized'].tolist()\n",
    "corpus_docs_val = X_val_text['custom_tokenized'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8213290-596d-4248-8902-53d18f851c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69e01cda-8f19-423c-9f6c-320d376bebf9",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0186919b-47ab-4e2e-b84f-0adcee5b7cf8",
   "metadata": {},
   "source": [
    "### Training fresh Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "592b351a-859e-4dfa-bd1a-592f07f89131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_docs_train)]\n",
    "model = Doc2Vec(documents, vector_size=50, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6ddd0-5ea2-4821-a93c-40a71ea30156",
   "metadata": {},
   "source": [
    "### Checking vector quality\n",
    "### By seeing similar vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd8666b-31c7-47d2-a014-000ff3aa3daa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26839852,  0.12887974,  0.0630647 ,  0.03264263,  0.3089021 ,\n",
       "        0.01702114,  0.7274753 , -0.30822697, -0.6050099 ,  0.15227705,\n",
       "        0.35476813, -0.43927732,  0.09672307, -0.52816296,  0.04128575,\n",
       "        0.11355636,  0.46911326,  0.12250529, -0.70135015, -0.44843572,\n",
       "       -0.5267816 ,  0.05371866,  0.23688714,  0.13769387,  0.20576397,\n",
       "        0.08601218,  0.36132756, -0.1707585 , -0.11482327, -0.21785106,\n",
       "       -0.15644243,  0.1730717 ,  0.17125547,  0.08009847, -0.33285233,\n",
       "        0.36002773, -0.49833554, -0.36968857,  0.4414048 , -0.3518874 ,\n",
       "       -0.09212486, -0.230916  , -0.52695817, -0.28784436,  0.36132815,\n",
       "        0.18354204, -0.03444148, -0.34004754, -0.12779355,  0.38095567],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model.infer_vector(corpus_docs_val[0])\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32de2b7d-1bcf-4c2c-893c-72b113739371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suket\\AppData\\Local\\Temp\\ipykernel_4188\\4040845599.py:1: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  similar_doc = model.docvecs.most_similar([v1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1864, 0.744713544845581),\n",
       " (241, 0.7161760330200195),\n",
       " (595, 0.7109609842300415),\n",
       " (946, 0.7107124328613281),\n",
       " (99, 0.7017166614532471),\n",
       " (2979, 0.6986605525016785),\n",
       " (1817, 0.697917640209198),\n",
       " (2981, 0.6946759223937988),\n",
       " (893, 0.6944105625152588),\n",
       " (1716, 0.6930670738220215)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_doc = model.docvecs.most_similar([v1])\n",
    "similar_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d182145-1792-4e50-b76a-df71613a1703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonoscopy is tool of choice for preventing Colorectal Cancer, by detecting and removing polyps before they become cancerous. However, colonoscopy is hampered by the fact that endoscopists routinely miss 22-28% of polyps. While some of these missed polyps appear in the endoscopist's field of view, others are missed simply because of substandard coverage of the procedure, i.e. not all of the colon is seen. This paper attempts to rectify the problem of substandard coverage in colonoscopy through the introduction of the C2D2 (Colonoscopy Coverage Deficiency via Depth) algorithm which detects deficient coverage, and can thereby alert the endoscopist to revisit a given area. More specifically, C2D2 consists of two separate algorithms: the first performs depth estimation of the colon given an ordinary RGB video stream; while the second computes coverage given these depth estimates. Rather than compute coverage for the entire colon, our algorithm computes coverage locally, on a segment-by-segment basis; C2D2 can then indicate in real-time whether a particular area of the colon has suffered from deficient coverage, and if so the endoscopist can return to that area. Our coverage algorithm is the first such algorithm to be evaluated in a large-scale way; while our depth estimation technique is the first calibration-free unsupervised method applied to colonoscopies. The C2D2 algorithm achieves state of the art results in the detection of deficient coverage. On synthetic sequences with ground truth, it is 2.4 times more accurate than human experts; while on real sequences, C2D2 achieves a 93.0% agreement with experts. \n",
      "\n",
      "Both acquisition and reconstruction speed are crucial for magnetic resonance (MR) imaging in clinical applications. In this paper, we present a fast reconstruction algorithm for SENSE in partially parallel MR imaging with arbitrary k-space trajectories. The proposed method is a combination of variable splitting, the classical penalty technique and the optimal gradient method. Variable splitting and the penalty technique reformulate the SENSE model with sparsity regularization as an unconstrained minimization problem, which can be solved by alternating two simple minimizations: One is the total variation and wavelet based denoising that can be quickly solved by several recent numerical methods, whereas the other one involves a linear inversion which is solved by the optimal first order gradient method in our algorithm to significantly improve the performance. Comparisons with several recent parallel imaging algorithms indicate that the proposed method significantly improves the computation efficiency and achieves state-of-the-art reconstruction quality. 0.744713544845581 \n",
      "\n",
      "Interest in RGB-D devices is increasing due to their low price and the wide range of possible applications that come along. These devices provide a marker-less body pose estimation by means of skeletal data consisting of 3D positions of body joints. These can be further used for pose, gesture or action recognition. In this work, an evolutionary algorithm is used to determine the optimal subset of skeleton joints, taking into account the topological structure of the skeleton, in order to improve the final success rate. The proposed method has been validated using a state-of-the-art RGB action recognition approach, and applying it to the MSR-Action3D dataset. Results show that the proposed algorithm is able to significantly improve the initial recognition rate and to yield similar or better success rates than the state-of-the-art methods. (C) 2013 Elsevier Ltd. All rights reserved. 0.7161760330200195 \n",
      "\n",
      "Deformable surface 3D tracking is a severely under-constrained problem and great efforts have been made to solve it. A recent state-of-the-art approach solves this problem by formulating it as a second order cone programming (SOCP) problem. However, one drawback of this approach is that it is time-consuming. In this paper, we propose an effective method for 3D deformable surface tracking. First, we formulate the deformable surface tracking problem as a linear programming (LP) problem. Then, we solve the LP problem with an algorithm which converges superlinearly rather than bisection algorithm whose convergence speed is linear. Our experimental studies on synthetic and real data have demonstrated the proposed method can not only reliably recover 3D structures of surfaces but also run faster than the state-of-the-art method. (C) 2011 Elsevier Ltd. All rights reserved. 0.7109609842300415 \n",
      "\n",
      "Despite the previous efforts of object proposals, the detection rates of the existing approaches are still not satisfactory enough. To address this, we propose Adobe Boxes to efficiently locate the potential objects with fewer proposals, in terms of searching the object adobes that are the salient object parts easy to be perceived. Because of the visual difference between the object and its surroundings, an object adobe obtained from the local region has a high probability to be a part of an object, which is capable of depicting the locative information of the proto-object. Our approach comprises of three main procedures. First, the coarse object proposals are acquired by employing randomly sampled windows. Then, based on local-contrast analysis, the object adobes are identified within the enlarged bounding boxes that correspond to the coarse proposals. The final object proposals are obtained by converging the bounding boxes to tightly surround the object adobes. Meanwhile, our object adobes can also refine the detection rate of most state-of-the-art methods as a refinement approach. The extensive experiments on four challenging datasets (PASCAL VOC2007, VOC2010, VOC2012, and ILSVRC2014) demonstrate that the detection rate of our approach generally outperforms the state-of-the-art methods, especially with relatively small number of proposals. The average time consumed on one image is about 48 ms, which nearly meets the real-time requirement. 0.7107124328613281 \n",
      "\n",
      "In this paper, we propose a novel biomedical cyber-physical system for automated and efficient arrhythmia and seizure detection in the time-series biomedical signals such as electrocardiogram (ECG) and electroencephalography (EEG). We use a novel multilayer, automated, and multistage deep residual network for the anomaly detection in the biomedical signals. Generally, the biomedical datasets have class imbalance problem; hence, we leverage the concepts of undersampling techniques to address this issue. The proposed algorithm is validated on the publicly available benchmark MIT-BIH Arrhythmia and CHB-MIT Scalp databases. The results show a significant improvement in terms of the sensitivity of 90% and 97.1% for supraventricular and ventricular beats for best fold, respectively. The accuracy obtained is at par with most of the state-of-the-art methods, and in particular, for the supraventricular beats, the proposed method outperforms all but one state-of-the-art method. The advantage of the proposed method is that it gives reliable results with EEG samples of small duration and, as opposed to other state-of-the-artmethods, it does not involve any preprocessing, hence computationally efficient. Additionally, the proposed algorithm provides 81% sensitivity for seizure detection in EEG signals, which is comparable to existing deep learning methods. 0.7017166614532471 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_val_text.iloc[0]['abstract'], '\\n')\n",
    "\n",
    "print(X_train_text.iloc[similar_doc[0][0]]['abstract'], similar_doc[0][1], '\\n')\n",
    "print(X_train_text.iloc[similar_doc[1][0]]['abstract'], similar_doc[1][1], '\\n')\n",
    "print(X_train_text.iloc[similar_doc[2][0]]['abstract'], similar_doc[2][1], '\\n')\n",
    "print(X_train_text.iloc[similar_doc[3][0]]['abstract'], similar_doc[3][1], '\\n')\n",
    "print(X_train_text.iloc[similar_doc[4][0]]['abstract'], similar_doc[4][1], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ea2bf4-01d8-4581-bd55-57d05874459c",
   "metadata": {},
   "source": [
    "## Fitting the trained Doc2Vec on both training and validation documents corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d3d9235-24ea-4187-b565-186cff004785",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = [model.infer_vector(doc) for doc in X_train_text['custom_tokenized'].tolist()]\n",
    "X_val_vec = [model.infer_vector(doc) for doc in X_val_text['custom_tokenized'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ec371-9d51-42ee-8c00-0d5393262bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f540002-751e-4af0-89e7-9888881f148e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['dermoid', 'cyst', 'also', 'called', 'mature', 'teratoma', 'benign', 'tumor', 'ovary', 'derived', 'pluripotent', 'germ', 'cells', 'often', 'asymptomatic', 'however', 'expressed', 'several', 'complications', 'including', 'infection', 'adnexal', 'torsion', 'rupture', 'rarely', 'ovarian', 'dermoid', 'cysts', 'also', 'transform', 'malignant', 'degeneration', 'ruptured', 'teratoma', 'rare', 'life', 'threatening', 'complication', 'may', 'arise', 'spontaneously', 'however', 'cystic', 'rupture', 'often', 'secondary', 'surgical', 'procedures', 'ovarian', 'cystectomy', 'leading', 'acute', 'peritonitis', 'surgical', 'emergency', 'herein', 'report', 'case', 'acute', 'peritonitis', 'female', 'resulting', 'ovarian', 'dermoid', 'cyst', 'spillage', 'clinical', 'picture', 'radiological', 'imaging', 'consistent', 'ruptured', 'ovarian', 'cyst', 'leading', 'chemical', 'peritonitis', 'histopathological', 'examination', 'confirmed', 'ovarian', 'dermoid', 'cyst'], tags=[3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bed81c-bb1d-45d5-b9bf-aac6a2394b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bccb956-f240-416e-91da-3e8ad92a2fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab176ad4-866e-4fa8-a9c5-0ec44d18373e",
   "metadata": {},
   "source": [
    "# ML Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7fdb095-a59d-4f04-84a5-5c48101129e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suket\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1300: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train_vec, Y_train)\n",
    "Y_pred = logreg.predict(X_val_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e228c0f-2cb5-44f2-9972-87690aec8668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy 0.8391666666666666\n",
      "Validation F1 score: 0.8388459599946492\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print('Validation accuracy %s' % accuracy_score(Y_val, Y_pred))\n",
    "print('Validation F1 score: {}'.format(f1_score(Y_val, Y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d277a-e0c6-4e67-8211-cf1723ee1c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd1f14c-943a-4de3-ac42-09faec9a28f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40df8c41-859a-4776-a2ca-85ab28896584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec2287d3-80cb-43e8-a37a-84486b2a8561",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codestop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcodestop\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'codestop' is not defined"
     ]
    }
   ],
   "source": [
    "codestop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9ccaa-a7a2-4f58-860e-fa5616a44169",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a11ff-5e75-4626-ba4e-45d56413f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd31fb-c53e-4a8a-b539-b8b4a2e9ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_pt_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ecec7-b0a8-4974-bf12-69773b9e7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_pt_vectors.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe38f2c-3667-46cf-97dd-9ab822741e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus_docs_train)]\n",
    "# model_d2v_tl = w2v_pt_vectors.wv(documents, vector_size=300, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ec580-55fa-4bb0-a0d9-be386095bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_pt_vectors['diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ea9c80-68a3-4656-9f1f-a88eedce4738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d080bc-603e-4cdf-a294-392ebb73915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "w2v_custom_model = Word2Vec(vector_size=300, min_count=1, window=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6176e-7c38-4b50-a2ee-222b11020746",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(w2v_custom_model.wv['diagnosis'])\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f75ab-2fff-4963-9fc8-72ae2c6a160a",
   "metadata": {},
   "source": [
    "#### Build vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a384eb3-c011-44f1-94e7-f4fd1acb1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_custom_model.build_vocab(corpus_docs_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0f7265-55da-4e73-b419-f2021e7d50e4",
   "metadata": {},
   "source": [
    "### Injecting pre-trained vectors into blank custom vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37928186-bceb-421e-872a-d5e8e1cf7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_custom_model = [w2v_custom_model.wv.index_to_key[i] for i in range(len(w2v_custom_model.wv))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cf4a0a-1eb4-4226-9074-f4a671a46b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_pt_model = [w2v_pt_vectors.index_to_key[i] for i in range(len(w2v_pt_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d98864-5fc9-4aff-9358-6d3600682bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in vocab_custom_model:\n",
    "    try:\n",
    "        # If a key is present in both pretrained and custom blank model\n",
    "        # Adopt the vector of pretrained model\n",
    "        w2v_custom_model.wv[key] = w2v_pt_vectors[key]\n",
    "    except:\n",
    "        # Else ignore and proceed ahead\n",
    "        # Because some corpus-specific workds 'might' not be present in the pretrained vector vocabulary\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a52a3b-ea16-474e-9824-d0a391c2141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(w2v_custom_model.wv['diagnosis'])\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c00a1-24cd-441d-bd63-7fde91b07fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5f563f8-c40f-412f-83c6-74884004d4c1",
   "metadata": {},
   "source": [
    "#### Instruct the model to update all vectors during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38018ea-d85a-4a37-a9d5-df5a022c8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_custom_model.wv.vectors_lockf = np.ones(len(w2v_custom_model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8638683b-eff5-4d46-902c-b74f67ce8121",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(w2v_custom_model.wv['diagnosis'])\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d90826-42c0-49c5-8819-d2ae411deea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_custom_model.train(corpus_docs_train, total_examples=len(corpus_docs_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1098b4-5d86-4829-a69f-82919d9e4034",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(w2v_custom_model.wv['diagnosis'])\n",
    "except KeyError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b509c-dee6-41d3-af92-4e1500bd8e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34690de-799f-4818-9765-271c842ad165",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = corpus_docs_val[0]\n",
    "test_doc = [x for x in test_doc if x in w2v_custom_model.wv]\n",
    "\n",
    "\n",
    "v1 = w2v_custom_model.wv[test_doc]\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed861552-b839-4da0-a3a2-807843cec731",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_doc = model.docvecs.most_similar([v1])\n",
    "similar_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc80a5-d96f-4086-b1e4-b5e854ca60ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d4501b-38d5-433d-b0cf-9974623fd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_custom_model.wv.index_to_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf176f-b224-4025-bc75-6983912bbe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_pt_vectors.index_to_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a024ec-0c7b-481a-a512-3fbe6c775ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65404895-d88d-4907-956c-8e6b22da08e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
